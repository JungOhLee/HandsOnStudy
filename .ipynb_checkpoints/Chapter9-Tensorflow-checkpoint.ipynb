{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 설치\n",
    "## 계산그래프 만들기, 세션 실행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 계산그래프 만들기\n",
    "x = tf.Variable(3, name=\"x\")\n",
    "y = tf.Variable(4, name=\"y\")\n",
    "f = x*x*y + y + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_1:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f # 값이 나오는 것이 아니고 그래프만 생성된 것이다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "# 값은 session을 실행해야 나온다. \n",
    "# session 에는 모든 변수값이 저장되어있다. (분산 처리에서는 서버에 저장)\n",
    "sess = tf.Session()\n",
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "result = sess.run(f)\n",
    "print(result) # 계산값이 나온다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close() # session 자원을 해제한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이렇게 하는 것보다 다음 코드가 더 쉽다. \n",
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with 블록 안에서는 선언한 session이 기본 세션으로 지정되고, x.initializer.run()은 tf.get_default_session().run(x.initializer)를 호출하는 것과 동일하다. 동일하게 f.eval()은 tf.get_default_session().run(f)와 같다고 할 수 있다. \n",
    "세션은 with 블록이 끝나면 자동으로 종료된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "# 변수를 일일이 초기화 하는대신 global_variables_initializer() 사용 가능\n",
    "\n",
    "init = tf.global_variables_initializer() # init 노드를 만든것임\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run() # 모든 변수 초기화\n",
    "    result = f.eval()\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위 작업을 shell에서 할 때는 tf.InteractiveSession()을 이용하면 저절로 기본 세션으로 지정되어 비슷한 작업을 할 수 있다. \n",
    "# 다만 끝날 때 close 해줘야 한다.\n",
    "\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "init.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = f.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 계산 그래프 관리\n",
    "\n",
    "노드를 만들면 자동으로 기본 계산 그래프에 추가됨\n",
    "\n",
    "하지만 다른 그래프를 만들어서 노드를 추가하는 것도 가능 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = tf.Variable(1) # 기본 그래프에 노드 추가\n",
    "x1.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = tf.Graph() # 새 그래프 생성\n",
    "with graph.as_default():\n",
    "    x2 = tf.Variable(2) # 새로 만든 그래프에 노드가 추가됨\n",
    "    \n",
    "x2.graph is graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 노드 값의 생애 주기\n",
    "노드 값은 그래프 계산 한 번 하는 동안만 유지됨\n",
    "\n",
    "변수 값은 세션에 의해 따로 저장되기 때문에, 세션 없애지 않는 한 유지됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "w = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5\n",
    "z = x * 3\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(y.eval())\n",
    "    print(z.eval()) # 그래프 계산간에 노드값이 유지가 안되기 때문에,y 값 구할때, x, w 연산 한 번 구해지고, z값 구할 때 또 x,w구해진다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# 다음 코드는 한 번의 연산으로 y와 z를 구한다. \n",
    "with tf.Session() as sess:\n",
    "    y_val, z_val = sess.run([y, z])\n",
    "    print(y_val)\n",
    "    print(z_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 선형회귀\n",
    "\n",
    "상수와 변수 연산은 입력이 없음 = source ops 라고 부름\n",
    "\n",
    "나머지 연산은 입력과 출력이 모두 있는데, 그것을 텐서라고 부른다. 엄청 다차원까지 가능하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /Users/JOLEE/scikit_learn_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': array([[   8.3252    ,   41.        ,    6.98412698, ...,    2.55555556,\n",
      "          37.88      , -122.23      ],\n",
      "       [   8.3014    ,   21.        ,    6.23813708, ...,    2.10984183,\n",
      "          37.86      , -122.22      ],\n",
      "       [   7.2574    ,   52.        ,    8.28813559, ...,    2.80225989,\n",
      "          37.85      , -122.24      ],\n",
      "       ..., \n",
      "       [   1.7       ,   17.        ,    5.20554273, ...,    2.3256351 ,\n",
      "          39.43      , -121.22      ],\n",
      "       [   1.8672    ,   18.        ,    5.32951289, ...,    2.12320917,\n",
      "          39.43      , -121.32      ],\n",
      "       [   2.3886    ,   16.        ,    5.25471698, ...,    2.61698113,\n",
      "          39.37      , -121.24      ]]), 'target': array([ 4.526,  3.585,  3.521, ...,  0.923,  0.847,  0.894]), 'feature_names': ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude'], 'DESCR': 'California housing dataset.\\n\\nThe original database is available from StatLib\\n\\n    http://lib.stat.cmu.edu/datasets/\\n\\nThe data contains 20,640 observations on 9 variables.\\n\\nThis dataset contains the average house value as target variable\\nand the following input variables (features): average income,\\nhousing average age, average rooms, average bedrooms, population,\\naverage occupation, latitude, and longitude in that order.\\n\\nReferences\\n----------\\n\\nPace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\\nStatistics and Probability Letters, 33 (1997) 291-297.\\n\\n'}\n",
      "[[   1.            8.3252       41.         ...,    2.55555556   37.88\n",
      "  -122.23      ]\n",
      " [   1.            8.3014       21.         ...,    2.10984183   37.86\n",
      "  -122.22      ]\n",
      " [   1.            7.2574       52.         ...,    2.80225989   37.85\n",
      "  -122.24      ]\n",
      " ..., \n",
      " [   1.            1.7          17.         ...,    2.3256351    39.43\n",
      "  -121.22      ]\n",
      " [   1.            1.8672       18.         ...,    2.12320917   39.43\n",
      "  -121.32      ]\n",
      " [   1.            2.3886       16.         ...,    2.61698113   39.37\n",
      "  -121.24      ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "print(housing)\n",
    "m, n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m,1)), housing.data]\n",
    "print(housing_data_plus_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype = tf.float32, name=\"y\")\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.74651413e+01]\n",
      " [  4.35734153e-01]\n",
      " [  9.33829229e-03]\n",
      " [ -1.06622010e-01]\n",
      " [  6.44106984e-01]\n",
      " [ -4.25131839e-06]\n",
      " [ -3.77322501e-03]\n",
      " [ -4.26648885e-01]\n",
      " [ -4.40514028e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(theta_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 경사하강법 구현\n",
    "\n",
    "정규화 안하면 오래걸리니까 꼭 해라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  2.34476576,  0.98214266, ..., -0.04959654,\n",
       "         1.05254828, -1.32783522],\n",
       "       [ 0.        ,  2.33223796, -0.60701891, ..., -0.09251223,\n",
       "         1.04318455, -1.32284391],\n",
       "       [ 0.        ,  1.7826994 ,  1.85618152, ..., -0.02584253,\n",
       "         1.03850269, -1.33282653],\n",
       "       ..., \n",
       "       [ 0.        , -1.14259331, -0.92485123, ..., -0.0717345 ,\n",
       "         1.77823747, -0.8237132 ],\n",
       "       [ 0.        , -1.05458292, -0.84539315, ..., -0.09122515,\n",
       "         1.77823747, -0.87362627],\n",
       "       [ 0.        , -0.78012947, -1.00430931, ..., -0.04368215,\n",
       "         1.75014627, -0.83369581]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정규화\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(housing_data_plus_bias)\n",
    "scaled_housing_data_plus_bias = scaler.transform(housing_data_plus_bias)\n",
    "scaled_housing_data_plus_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 14.2197\n",
      "Epoch 1000 MSE = 4.81004\n",
      "Epoch 2000 MSE = 4.80378\n",
      "Epoch 3000 MSE = 4.80332\n",
      "Epoch 4000 MSE = 4.80326\n",
      "Epoch 5000 MSE = 4.80326\n",
      "Epoch 6000 MSE = 4.80326\n",
      "Epoch 7000 MSE = 4.80326\n",
      "Epoch 8000 MSE = 4.80325\n",
      "Epoch 9000 MSE = 4.80325\n",
      "[[-0.63042879]\n",
      " [ 0.82963645]\n",
      " [ 0.11875467]\n",
      " [-0.26555768]\n",
      " [ 0.30572116]\n",
      " [-0.00450216]\n",
      " [-0.03932696]\n",
      " [-0.89984661]\n",
      " [-0.87050402]]\n"
     ]
    }
   ],
   "source": [
    "# 그래디언트 직접 계산 \n",
    "\n",
    "n_epochs = 10000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype = tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype = tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name = \"theta\")\n",
    "y_pred = tf.matmul(X, theta, name = \"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name = \"mse\")\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error) # 수식에 의해서 구한 방법\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients) # 값 update\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess: \n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 1000 ==0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "        \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "print(best_theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"gradients_1/predictions_9_grad/MatMul_1:0\", shape=(9, 1), dtype=float32)\n",
      "Epoch 0 MSE = 8.01569\n",
      "Epoch 1000 MSE = 4.81396\n",
      "Epoch 2000 MSE = 4.80454\n",
      "Epoch 3000 MSE = 4.80344\n",
      "Epoch 4000 MSE = 4.80329\n",
      "Epoch 5000 MSE = 4.80325\n",
      "Epoch 6000 MSE = 4.80325\n",
      "Epoch 7000 MSE = 4.80325\n",
      "Epoch 8000 MSE = 4.80326\n",
      "Epoch 9000 MSE = 4.80325\n",
      "[[ 0.71013808]\n",
      " [ 0.82964802]\n",
      " [ 0.11875715]\n",
      " [-0.2655803 ]\n",
      " [ 0.30574027]\n",
      " [-0.00450123]\n",
      " [-0.03932739]\n",
      " [-0.89981824]\n",
      " [-0.87047791]]\n"
     ]
    }
   ],
   "source": [
    "# 자동 미분 사용\n",
    "# 텐서플로는 reverse-mode autodiff 를 사용\n",
    "\n",
    "n_epochs = 10000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype = tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype = tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name = \"theta\")\n",
    "y_pred = tf.matmul(X, theta, name = \"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name = \"mse\")\n",
    "gradients = tf.gradients(mse, [theta])[0] # 변수를 리스트로 받아서, 각 변수에 대한 gradient를 계산하는 연산을 자동으로 만들음\n",
    "print(gradients)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients) # 값 update\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess: \n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 1000 ==0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "        \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "print(best_theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.08983\n",
      "Epoch 1000 MSE = 0.529381\n",
      "Epoch 2000 MSE = 0.524583\n",
      "Epoch 3000 MSE = 0.524342\n",
      "Epoch 4000 MSE = 0.524323\n",
      "Epoch 5000 MSE = 0.524321\n",
      "Epoch 6000 MSE = 0.524321\n",
      "Epoch 7000 MSE = 0.524321\n",
      "Epoch 8000 MSE = 0.524321\n",
      "Epoch 9000 MSE = 0.524321\n",
      "[[ 2.06855249]\n",
      " [ 0.82963336]\n",
      " [ 0.11875444]\n",
      " [-0.26555103]\n",
      " [ 0.30571586]\n",
      " [-0.00450222]\n",
      " [-0.03932687]\n",
      " [-0.89985251]\n",
      " [-0.87050939]]\n"
     ]
    }
   ],
   "source": [
    "# 옵티마이저 사용\n",
    "# gradient 와 training_op 에서 assign 하던 과정을 한 번에 해줌\n",
    "\n",
    "n_epochs = 10000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype = tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype = tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name = \"theta\")\n",
    "y_pred = tf.matmul(X, theta, name = \"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name = \"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess: \n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 1000 ==0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "        \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "print(best_theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련 알고리즘에 데이터 주입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 8.09276\n",
      "Epoch 10 MSE = 0.38377\n",
      "Epoch 20 MSE = 0.386213\n",
      "Epoch 30 MSE = 0.386364\n",
      "Epoch 40 MSE = 0.387226\n",
      "Epoch 50 MSE = 0.388277\n",
      "Epoch 60 MSE = 0.389235\n",
      "Epoch 70 MSE = 0.390014\n",
      "Epoch 80 MSE = 0.390609\n",
      "Epoch 90 MSE = 0.391047\n",
      "[[ 2.06083608]\n",
      " [ 0.82682461]\n",
      " [ 0.12791054]\n",
      " [-0.25998065]\n",
      " [ 0.30065781]\n",
      " [-0.0065838 ]\n",
      " [-0.03739018]\n",
      " [-0.86550617]\n",
      " [-0.85783559]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 배치(batch)를 사용하려면, 입력값을 자유롭게 변할 수 있게 해야함 -> placeholder 사용\n",
    "X = tf.placeholder(tf.float32, shape=(None, n+1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\") # None 하면 어떤 차원이던 가능\n",
    "\n",
    "n_epochs = 100\n",
    "learning_rate = 0.01\n",
    "batch_size = 1000\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name = \"theta\")\n",
    "y_pred = tf.matmul(X, theta, name = \"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name = \"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "target = housing.target.reshape(-1, 1)\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    m,n = np.shape(scaled_housing_data_plus_bias)\n",
    "    start = batch_size * batch_index\n",
    "    if start > m - batch_size:\n",
    "        end = m + 1\n",
    "    else: \n",
    "        end = start + batch_size\n",
    "            \n",
    "    X_batch = scaled_housing_data_plus_bias[start:end]\n",
    "    y_batch = target[start:end]\n",
    "    return X_batch, y_batch\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 10 ==0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            \n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 저장과 복원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 6.23894\n",
      "Epoch 300 MSE = 0.577208\n",
      "Epoch 600 MSE = 0.546824\n",
      "Epoch 900 MSE = 0.534331\n",
      "Epoch 1200 MSE = 0.528984\n",
      "Epoch 1500 MSE = 0.526594\n",
      "Epoch 1800 MSE = 0.525477\n",
      "Epoch 2100 MSE = 0.52493\n",
      "Epoch 2400 MSE = 0.524651\n",
      "Epoch 2700 MSE = 0.524503\n",
      "[[-0.80818677]\n",
      " [ 0.13425922]\n",
      " [-0.26258779]\n",
      " [ 0.66848898]\n",
      " [-0.06676865]\n",
      " [ 0.43106151]\n",
      " [ 0.34694457]\n",
      " [ 0.43212795]\n",
      " [ 0.89650083]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# saver 활용 - 잘 안되는군..\n",
    "\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name = \"theta\")\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver({\"weights\": theta})\n",
    "n_epochs = 3000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 300 ==0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval(feed_dict={X: scaled_housing_data_plus_bias, y: housing.target.reshape(-1, 1)}))\n",
    "            save_path = saver.save(sess, \"./tmp/my_model.ckpt\")\n",
    "        sess.run(training_op, feed_dict={X: scaled_housing_data_plus_bias, y: housing.target.reshape(-1, 1)})\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    print(best_theta)\n",
    "    save_path = saver.save(sess, \"./tmp/my_model_final.ckpt\")\n",
    "    \n",
    "# 값 결과가 이상하게 나오는군..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tmp/my_model_final.ckpt\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value theta_20\n\t [[Node: theta_20/read = Identity[T=DT_FLOAT, _class=[\"loc:@theta_20\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](theta_20)]]\n\nCaused by op 'theta_20/read', defined at:\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-78-8e9b8fbdb1eb>\", line 11, in <module>\n    theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name = \"theta\")\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 213, in __init__\n    constraint=constraint)\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 356, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 125, in identity\n    return gen_array_ops.identity(input, name=name)\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2071, in identity\n    \"Identity\", input=input, name=name)\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value theta_20\n\t [[Node: theta_20/read = Identity[T=DT_FLOAT, _class=[\"loc:@theta_20\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](theta_20)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value theta_20\n\t [[Node: theta_20/read = Identity[T=DT_FLOAT, _class=[\"loc:@theta_20\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](theta_20)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-011a875b4284>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m300\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MSE =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscaled_housing_data_plus_bias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhousing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./tmp/my_model.ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscaled_housing_data_plus_bias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhousing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m     \"\"\"\n\u001b[0;32m--> 570\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_dup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   4453\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4454\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 4455\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value theta_20\n\t [[Node: theta_20/read = Identity[T=DT_FLOAT, _class=[\"loc:@theta_20\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](theta_20)]]\n\nCaused by op 'theta_20/read', defined at:\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-78-8e9b8fbdb1eb>\", line 11, in <module>\n    theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name = \"theta\")\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 213, in __init__\n    constraint=constraint)\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 356, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 125, in identity\n    return gen_array_ops.identity(input, name=name)\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2071, in identity\n    \"Identity\", input=input, name=name)\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/Users/JOLEE/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value theta_20\n\t [[Node: theta_20/read = Identity[T=DT_FLOAT, _class=[\"loc:@theta_20\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](theta_20)]]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 3000\n",
    "saver = tf.train.import_meta_graph(\"./tmp/my_model.ckpt.meta\")\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./tmp/my_model_final.ckpt\")\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 300 ==0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", \n",
    "                  mse.eval(feed_dict={X: scaled_housing_data_plus_bias, y: housing.target.reshape(-1, 1)}))\n",
    "            save_path = saver.save(sess, \"./tmp/my_model.ckpt\")\n",
    "        sess.run(training_op, \n",
    "                 feed_dict={X: scaled_housing_data_plus_bias, y: housing.target.reshape(-1, 1)})\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    print(best_theta)\n",
    "    save_path = saver.save(sess, \"./tmp/my_model_final.ckpt\")\n",
    "    \n",
    "# 값 결과가 이상하게 나오는군..?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서보드 그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = housing.target.reshape(-1, 1)\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    m,n = np.shape(scaled_housing_data_plus_bias)\n",
    "    start = batch_size * batch_index\n",
    "    if start > m - batch_size:\n",
    "        end = m + 1\n",
    "    else: \n",
    "        end = start + batch_size\n",
    "            \n",
    "    X_batch = scaled_housing_data_plus_bias[start:end]\n",
    "    y_batch = target[start:end]\n",
    "    return X_batch, y_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.0194\n",
      "Epoch 10 MSE = 0.429247\n",
      "Epoch 20 MSE = 0.408771\n",
      "Epoch 30 MSE = 0.399009\n",
      "Epoch 40 MSE = 0.394753\n",
      "Epoch 50 MSE = 0.39305\n",
      "Epoch 60 MSE = 0.392471\n",
      "Epoch 70 MSE = 0.392348\n",
      "Epoch 80 MSE = 0.39238\n",
      "Epoch 90 MSE = 0.392442\n",
      "[[ 2.06130505]\n",
      " [ 0.84117979]\n",
      " [ 0.13187985]\n",
      " [-0.28387734]\n",
      " [ 0.319112  ]\n",
      " [-0.00516622]\n",
      " [-0.03802644]\n",
      " [-0.82302493]\n",
      " [-0.81627142]]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}\".format(root_logdir, now)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n+1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\") # None 하면 어떤 차원이던 가능\n",
    "\n",
    "n_epochs = 100\n",
    "learning_rate = 0.01\n",
    "batch_size = 1000\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name = \"theta\")\n",
    "y_pred = tf.matmul(X, theta, name = \"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name = \"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 10 ==0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            if batch_index %10:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "file_writer.close()\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이름 범위\n",
    "\n",
    "노드를 보기 쉽게, 관련 있는 것들은 묶어보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 5.85654\n",
      "Epoch 10 MSE = 0.545414\n",
      "Epoch 20 MSE = 0.479297\n",
      "Epoch 30 MSE = 0.443983\n",
      "Epoch 40 MSE = 0.424262\n",
      "Epoch 50 MSE = 0.412931\n",
      "Epoch 60 MSE = 0.406198\n",
      "Epoch 70 MSE = 0.40204\n",
      "Epoch 80 MSE = 0.399364\n",
      "Epoch 90 MSE = 0.397568\n",
      "[[ 2.06212401]\n",
      " [ 0.86780208]\n",
      " [ 0.13772976]\n",
      " [-0.33114511]\n",
      " [ 0.35707498]\n",
      " [-0.00308605]\n",
      " [-0.03910448]\n",
      " [-0.7549904 ]\n",
      " [-0.75015944]]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}\".format(root_logdir, now)\n",
    "\n",
    "tf.reset_default_graph() # 이거 안하면 그래프 여러 개 찍힘, 원래 있던거랑 합해서 ㅎㅎ\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n+1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\") # None 하면 어떤 차원이던 가능\n",
    "\n",
    "n_epochs = 100\n",
    "learning_rate = 0.01\n",
    "batch_size = 1000\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name = \"theta\")\n",
    "y_pred = tf.matmul(X, theta, name = \"predictions\")\n",
    "\n",
    "with tf.name_scope(\"loss\") as scope:\n",
    "    error = y_pred - y\n",
    "    mse = tf.reduce_mean(tf.square(error), name = \"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 10 ==0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            if batch_index %10:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "file_writer.close()\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss/sub\n",
      "loss/mse\n"
     ]
    }
   ],
   "source": [
    "# 이름이 하나의 계층 아래 묶인 것을 확인 가능\n",
    "print(error.op.name)\n",
    "print(mse.op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모듈화 \n",
    "\n",
    "$h_w,_b(X) = max(X \\cdot w + b, 0) $ 을 여러번 쓰려면 이를 모듈화 하는게 낫다. \n",
    "\n",
    "계산 그래프를 모듈화 하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    w_shape = (int(X.get_shape()[1]), 1)\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "    b = tf.Variable(0.0, name=\"bias\")\n",
    "    z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "    return tf.maximum(z, 0., name=\"relu\")\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name =\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}\".format(root_logdir, now)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "file_writer.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 이름 공간을 사용하면 훨씬 깔끔하게 표현 가능\n",
    "\n",
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "        return tf.maximum(z, 0., name=\"relu\")\n",
    "\n",
    "tf.reset_default_graph()\n",
    "    \n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name =\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}\".format(root_logdir, now)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 변수 공유"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수를 여러 함수에 공유하고 싶을 때\n",
    "\n",
    "def relu(X, threshold):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "        return tf.maximum(z, threshold, name=\"relu\")\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_features = 3\n",
    "threshold = tf.Variable(0.0, name=\"threshold\") # 변수를 생성해서 필요한 곳마다 넣어주면 자동으로 연결된다. \n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name =\"X\")\n",
    "relus = [relu(X, threshold) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}\".format(root_logdir, now)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에 처럼 하면 번거로울 때가 많다. \n",
    "# 많은 사람들이 딕셔너리로, 모델에 있는 모든 변수를 담아 한꺼번에 보낸다. \n",
    "# 함수를 만들때, 속성으로 공유변수를 지정해버리는 수도 있다. \n",
    "\n",
    "\n",
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        if not hasattr(relu, \"threshold\"):\n",
    "            relu.threshold = tf.Variable(0.0, name = \"threshold\")\n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "        return tf.maximum(z, relu.threshold, name=\"max\")\n",
    "        \n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_features = 3\n",
    "threshold = tf.Variable(0.0, name=\"threshold\") # 변수를 생성해서 필요한 곳마다 넣어주면 자동으로 연결된다. \n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name =\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}\".format(root_logdir, now)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에것 보다 좋은 방법은 get_variable() 함수로, 변수가 없으면 만들고, 있으면 재사용하는 것이다. \n",
    "\n",
    "변수를 재사용 할때는 variable scope의 reuse를 True 로 명시적으로 바꿔주어야한다. \n",
    "\n",
    "```\n",
    "with tf.variable_scope(\"relu\", reuse=True):\n",
    "threshold = tf.get_variable(\"threshold\")\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음과 같이 변수 범위 안에서 reuse 를 True로 바꿀 수도 있다. \n",
    "```\n",
    "with tf.variable_scope(\"relu\") as scope:\n",
    "    scope.reuse_variables()\n",
    "    threshold = tf.get_variable(\"threshold\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제 매개변수로 전달하지 말고 threshold 변수를 공유해보자! \n",
    "\n",
    "def relu(X):\n",
    "    with tf.variable_scope(\"relu\", reuse=True):\n",
    "        threshold = tf.get_variable(\"threshold\")\n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "        return tf.maximum(z, threshold, name=\"max\")\n",
    "        \n",
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "with tf.variable_scope(\"relu\"): # 이 변수 scope 내에서 변수를 선언해주는거군!!!!\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(), \n",
    "                                initializer=tf.constant_initializer(0.0))\n",
    "relus = [relu(X) for relu_index in range(5)]\n",
    "ouput = tf.add_n(relus, name=\"ouput\")\n",
    "    \n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}\".format(root_logdir, now)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에서는 relu 함수 밖에서 threshold 를 정의했지만, \n",
    "# 다음 코드에서는 relu 함수 내에서 아예 정의해버리는 수가 있다. \n",
    "# 이렇게 하면 이름 범위나 변수 공유를 신경쓰지 않아도 된다. \n",
    "\n",
    "def relu(X):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(),\n",
    "                               initializer=tf.constant_initializer(0.0))\n",
    "    w_shape = (int(X.get_shape()[1]), 1)\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "    b = tf.Variable(0.0, name=\"bias\")\n",
    "    z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "    return tf.maximum(z, threshold, name=\"max\")\n",
    "    \n",
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name =\"X\")\n",
    "relus=[]\n",
    "for relu_index in range(5):\n",
    "    with tf.variable_scope(\"relu\", reuse=(relu_index >= 1)) as scope:\n",
    "        relus.append(relu(X))\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}\".format(root_logdir, now)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 연습문제  12번"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=10000, noise=0.15, random_state=42)\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "class BatchFetcher:\n",
    "    def __init__(self, X, y):\n",
    "        self._current_epoch = 0\n",
    "        self._X = X\n",
    "        self._y = y\n",
    "        self._X_length = X.shape[0]\n",
    "        self._idx = np.arange(X.shape[0])\n",
    "        pass\n",
    "    \n",
    "    def fetch(self, epoch_idx, batch_idx, batch_size):\n",
    "        if epoch_idx != self._current_epoch:\n",
    "            self._current_epoch = epoch_idx\n",
    "            np.random.shuffle(self._idx)\n",
    "        start = batch_idx * batch_size\n",
    "        end = start + batch_size\n",
    "        if end > self._X_length:\n",
    "            end = self._X_length + 1\n",
    "        selected_idxs = self._idx[start:end]\n",
    "        X_batch = self._X[selected_idxs]\n",
    "        y_batch = self._y[selected_idxs]\n",
    "        return X_batch, y_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X):\n",
    "    with tf.variable_scope(\"logistic\", reuse=True) as scope:\n",
    "        theta = tf.get_variable(\"theta\")\n",
    "        b = tf.get_variable(\"bias\")\n",
    "        z = tf.add(tf.matmul(X, theta), b, name=\"z\")\n",
    "        output = tf.div(1., 1. + tf.exp(-z), name=\"output\")\n",
    "        return output\n",
    "\n",
    "n_epochs = 1000\n",
    "batch_size = 888\n",
    "max_batch_idx = int( np.ceil(X.shape[0] / batch_size) )\n",
    "learning_rate = 0.01\n",
    "    \n",
    "tf.reset_default_graph()\n",
    "\n",
    "X_input = tf.placeholder(dtype=tf.float32, shape = (None, X.shape[1]), name=\"X_input\")\n",
    "y_input = tf.placeholder(tf.float32, shape = (None, 1), name=\"y_input\")\n",
    "\n",
    "with tf.variable_scope(\"logistic\") as scope:\n",
    "    theta_dim = (int(X.shape[1]),1)\n",
    "    theta = tf.get_variable(\"theta\", shape = theta_dim, \n",
    "                            initializer=tf.random_normal_initializer())\n",
    "    b =tf.get_variable(\"bias\", shape = (), initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "y_pred = logistic_regression(X_input)\n",
    "with tf.name_scope(\"loss\") as scope: \n",
    "    error = y_input - y_pred\n",
    "    mse = tf.reduce_mean(tf.square(error))\n",
    "\n",
    "\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "training_op = optimizer.minimize(mse, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.69346684, -0.40213285],\n",
       "        [ 1.13703782, -0.28952817],\n",
       "        [ 1.5268697 , -0.52686092]]), array([[1],\n",
       "        [1],\n",
       "        [1]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetcher = BatchFetcher(X, y)\n",
    "fetcher.fetch(1,0,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 MSE =  0.178703\n",
      "Epoch: 100 MSE =  0.142862\n",
      "Epoch: 200 MSE =  0.129505\n",
      "Epoch: 300 MSE =  0.121208\n",
      "Epoch: 400 MSE =  0.115288\n",
      "Epoch: 500 MSE =  0.110918\n",
      "Epoch: 600 MSE =  0.107623\n",
      "Epoch: 700 MSE =  0.105095\n",
      "Epoch: 800 MSE =  0.103123\n",
      "Epoch: 900 MSE =  0.101564\n",
      "theta: [[ 0.96896684]\n",
      " [-2.7240746 ]]\n",
      "beta: 0.152329\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver({\"weight\": theta, \"bias\": b, \"step\": global_step})\n",
    "mse_summary = tf.summary.scalar(\"MSE\", mse)\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}\".format(root_logdir, now)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess: \n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch in range(max_batch_idx):\n",
    "            X_batch, y_batch = fetcher.fetch(epoch, batch, batch_size)\n",
    "            if batch % 10 ==0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X_input: X_batch, y_input: y_batch})\n",
    "                file_writer.add_summary(summary_str, tf.train.global_step(sess, global_step))\n",
    "            sess.run(training_op, feed_dict={X_input: X_batch, y_input: y_batch})\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            save_path = saver.save(sess, \"./tmp_save/save.ckpt\")\n",
    "            print(\"Epoch:\", epoch, \"MSE = \", \n",
    "                  mse.eval(feed_dict={X_input: X_batch, y_input: y_batch}))\n",
    "    save_path = saver.save(sess, \"./tmp_save/save_final.ckpt\")\n",
    "    best_theta = theta.eval()\n",
    "    print(\"theta:\", best_theta)\n",
    "    print(\"beta:\", b.eval())\n",
    "\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tmp_save/save_final.ckpt\n",
      "Epoch: 0 MSE =  0.100315\n",
      "Epoch: 100 MSE =  0.099303\n",
      "Epoch: 200 MSE =  0.098474\n",
      "Epoch: 300 MSE =  0.0977884\n",
      "Epoch: 400 MSE =  0.0972165\n",
      "Epoch: 500 MSE =  0.0967354\n",
      "Epoch: 600 MSE =  0.0963279\n",
      "Epoch: 700 MSE =  0.0959804\n",
      "Epoch: 800 MSE =  0.0956823\n",
      "Epoch: 900 MSE =  0.0954252\n",
      "theta: [[ 0.95867622]\n",
      " [-3.41459179]]\n",
      "beta: 0.357874\n"
     ]
    }
   ],
   "source": [
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess: \n",
    "    saver.restore(sess, \"./tmp_save/save_final.ckpt\")\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch in range(max_batch_idx):\n",
    "            X_batch, y_batch = fetcher.fetch(epoch, batch, batch_size)\n",
    "            if batch % 10 ==0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X_input: X_batch, y_input: y_batch})\n",
    "                file_writer.add_summary(summary_str, tf.train.global_step(sess, global_step))\n",
    "            sess.run(training_op, feed_dict={X_input: X_batch, y_input: y_batch})\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            save_path = saver.save(sess, \"./tmp_save/save.ckpt\")\n",
    "            print(\"Epoch:\", epoch, \"MSE = \", \n",
    "                  mse.eval(feed_dict={X_input: X_batch, y_input: y_batch}))\n",
    "    save_path = saver.save(sess, \"./tmp_save/save_final.ckpt\")\n",
    "    best_theta = theta.eval()\n",
    "    print(\"theta:\", best_theta)\n",
    "    print(\"beta:\", b.eval())\n",
    "\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tmp_save/save_final.ckpt\n",
      "Epoch: 0 MSE =  0.0952023\n",
      "Epoch: 100 MSE =  0.0950082\n",
      "Epoch: 200 MSE =  0.0948386\n",
      "Epoch: 300 MSE =  0.0946897\n",
      "Epoch: 400 MSE =  0.0945586\n",
      "Epoch: 500 MSE =  0.0944428\n",
      "Epoch: 600 MSE =  0.0943403\n",
      "Epoch: 700 MSE =  0.0942492\n",
      "Epoch: 800 MSE =  0.0941682\n",
      "Epoch: 900 MSE =  0.094096\n",
      "theta: [[ 0.97893214]\n",
      " [-3.78498936]]\n",
      "beta: 0.448826\n"
     ]
    }
   ],
   "source": [
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess: \n",
    "    saver.restore(sess, \"./tmp_save/save_final.ckpt\")\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch in range(max_batch_idx):\n",
    "            X_batch, y_batch = fetcher.fetch(epoch, batch, batch_size)\n",
    "            if batch % 10 ==0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X_input: X_batch, y_input: y_batch})\n",
    "                file_writer.add_summary(summary_str, tf.train.global_step(sess, global_step))\n",
    "            sess.run(training_op, feed_dict={X_input: X_batch, y_input: y_batch})\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            save_path = saver.save(sess, \"./tmp_save/save.ckpt\")\n",
    "            print(\"Epoch:\", epoch, \"MSE = \", \n",
    "                  mse.eval(feed_dict={X_input: X_batch, y_input: y_batch}))\n",
    "    save_path = saver.save(sess, \"./tmp_save/save_final.ckpt\")\n",
    "    best_theta = theta.eval()\n",
    "    print(\"theta:\", best_theta)\n",
    "    print(\"beta:\", b.eval())\n",
    "\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tmp_save/save_final.ckpt\n",
      "Epoch: 0 MSE =  0.0940316\n",
      "Epoch: 100 MSE =  0.0939739\n",
      "Epoch: 200 MSE =  0.0939222\n",
      "Epoch: 300 MSE =  0.0938759\n",
      "Epoch: 400 MSE =  0.0938343\n",
      "Epoch: 500 MSE =  0.0937969\n",
      "Epoch: 600 MSE =  0.0937633\n",
      "Epoch: 700 MSE =  0.0937331\n",
      "Epoch: 800 MSE =  0.093706\n",
      "Epoch: 900 MSE =  0.0936815\n",
      "theta: [[ 0.99757177]\n",
      " [-4.01951361]]\n",
      "beta: 0.502124\n"
     ]
    }
   ],
   "source": [
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess: \n",
    "    saver.restore(sess, \"./tmp_save/save_final.ckpt\")\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch in range(max_batch_idx):\n",
    "            X_batch, y_batch = fetcher.fetch(epoch, batch, batch_size)\n",
    "            if batch % 10 ==0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X_input: X_batch, y_input: y_batch})\n",
    "                file_writer.add_summary(summary_str, tf.train.global_step(sess, global_step))\n",
    "            sess.run(training_op, feed_dict={X_input: X_batch, y_input: y_batch})\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            save_path = saver.save(sess, \"./tmp_save/save.ckpt\")\n",
    "            print(\"Epoch:\", epoch, \"MSE = \", \n",
    "                  mse.eval(feed_dict={X_input: X_batch, y_input: y_batch}))\n",
    "    save_path = saver.save(sess, \"./tmp_save/save_final.ckpt\")\n",
    "    best_theta = theta.eval()\n",
    "    print(\"theta:\", best_theta)\n",
    "    print(\"beta:\", b.eval())\n",
    "\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tmp_save/save_final.ckpt\n",
      "Epoch: 0 MSE =  0.0936595\n",
      "Epoch: 100 MSE =  0.0936397\n",
      "Epoch: 200 MSE =  0.0936219\n",
      "Epoch: 300 MSE =  0.093606\n",
      "Epoch: 400 MSE =  0.0935916\n",
      "Epoch: 500 MSE =  0.0935787\n",
      "Epoch: 600 MSE =  0.0935672\n",
      "Epoch: 700 MSE =  0.0935569\n",
      "Epoch: 800 MSE =  0.0935477\n",
      "Epoch: 900 MSE =  0.0935394\n",
      "theta: [[ 1.01232731]\n",
      " [-4.18058586]]\n",
      "beta: 0.537331\n"
     ]
    }
   ],
   "source": [
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess: \n",
    "    saver.restore(sess, \"./tmp_save/save_final.ckpt\")\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch in range(max_batch_idx):\n",
    "            X_batch, y_batch = fetcher.fetch(epoch, batch, batch_size)\n",
    "            if batch % 10 ==0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X_input: X_batch, y_input: y_batch})\n",
    "                file_writer.add_summary(summary_str, tf.train.global_step(sess, global_step))\n",
    "            sess.run(training_op, feed_dict={X_input: X_batch, y_input: y_batch})\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            save_path = saver.save(sess, \"./tmp_save/save.ckpt\")\n",
    "            print(\"Epoch:\", epoch, \"MSE = \", \n",
    "                  mse.eval(feed_dict={X_input: X_batch, y_input: y_batch}))\n",
    "    save_path = saver.save(sess, \"./tmp_save/save_final.ckpt\")\n",
    "    best_theta = theta.eval()\n",
    "    print(\"theta:\", best_theta)\n",
    "    print(\"beta:\", b.eval())\n",
    "\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tmp_save/save_final.ckpt\n",
      "0.8797\n"
     ]
    }
   ],
   "source": [
    "# accuracy 계산\n",
    "with tf.Session() as sess: \n",
    "    saver.restore(sess, \"./tmp_save/save_final.ckpt\")\n",
    "    y_result = y_pred.eval(feed_dict={X_input: X})\n",
    "    y_result = np.round(y_result)\n",
    "    accu = np.sum(y_result == y) / y.shape[0]\n",
    "    print(accu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 MSE =  0.329817\n",
      "Epoch: 1000 MSE =  0.10477\n",
      "Epoch: 2000 MSE =  0.0932505\n",
      "Epoch: 3000 MSE =  0.0890738\n",
      "Epoch: 4000 MSE =  0.0870668\n",
      "Epoch: 5000 MSE =  0.0859442\n",
      "Epoch: 6000 MSE =  0.0852528\n",
      "Epoch: 7000 MSE =  0.0847983\n",
      "Epoch: 8000 MSE =  0.0844852\n",
      "Epoch: 9000 MSE =  0.0842616\n",
      "Epoch: 10000 MSE =  0.0840976\n",
      "Epoch: 11000 MSE =  0.0839746\n",
      "Epoch: 12000 MSE =  0.0838805\n",
      "Epoch: 13000 MSE =  0.0838076\n",
      "Epoch: 14000 MSE =  0.0837504\n",
      "Epoch: 15000 MSE =  0.0837049\n",
      "Epoch: 16000 MSE =  0.0836684\n",
      "Epoch: 17000 MSE =  0.0836389\n",
      "Epoch: 18000 MSE =  0.0836148\n",
      "Epoch: 19000 MSE =  0.0835951\n",
      "Epoch: 20000 MSE =  0.0835788\n",
      "Epoch: 21000 MSE =  0.0835652\n",
      "Epoch: 22000 MSE =  0.0835539\n",
      "Epoch: 23000 MSE =  0.0835444\n",
      "Epoch: 24000 MSE =  0.0835363\n",
      "Epoch: 25000 MSE =  0.0835295\n",
      "Epoch: 26000 MSE =  0.0835238\n",
      "Epoch: 27000 MSE =  0.0835189\n",
      "Epoch: 28000 MSE =  0.0835146\n",
      "Epoch: 29000 MSE =  0.083511\n",
      "theta: [[ 1.08411622]\n",
      " [-4.62936592]]\n",
      "beta: 0.59381\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_epochs = 30000\n",
    "batch_size = 2000\n",
    "max_batch_idx = int( np.ceil(X.shape[0] / batch_size) )\n",
    "learning_rate = 0.005\n",
    "    \n",
    "tf.reset_default_graph()\n",
    "\n",
    "X_input = tf.placeholder(dtype=tf.float32, shape = (None, X.shape[1]), name=\"X_input\")\n",
    "y_input = tf.placeholder(tf.float32, shape = (None, 1), name=\"y_input\")\n",
    "\n",
    "with tf.variable_scope(\"logistic\") as scope:\n",
    "    theta_dim = (int(X.shape[1]),1)\n",
    "    theta = tf.get_variable(\"theta\", shape = theta_dim, \n",
    "                            initializer=tf.random_normal_initializer())\n",
    "    b =tf.get_variable(\"bias\", shape = (), initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "y_pred = logistic_regression(X_input)\n",
    "with tf.name_scope(\"loss\") as scope: \n",
    "    error = y_input - y_pred\n",
    "    mse = tf.reduce_mean(tf.square(error))\n",
    "\n",
    "\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "training_op = optimizer.minimize(mse, global_step=global_step)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver({\"weight\": theta, \"bias\": b, \"step\": global_step})\n",
    "mse_summary = tf.summary.scalar(\"MSE\", mse)\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}\".format(root_logdir, now)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess: \n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch in range(max_batch_idx):\n",
    "            X_batch, y_batch = fetcher.fetch(epoch, batch, batch_size)\n",
    "            if batch % 5 ==0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X_input: X_batch, y_input: y_batch})\n",
    "                file_writer.add_summary(summary_str, tf.train.global_step(sess, global_step))\n",
    "            sess.run(training_op, feed_dict={X_input: X_batch, y_input: y_batch})\n",
    "        \n",
    "        if epoch % 1000 == 0:\n",
    "            save_path = saver.save(sess, \"./tmp_save/save.ckpt\")\n",
    "            print(\"Epoch:\", epoch, \"MSE = \", \n",
    "                  mse.eval(feed_dict={X_input: X_batch, y_input: y_batch}))\n",
    "    save_path = saver.save(sess, \"./tmp_save/save_final.ckpt\")\n",
    "    best_theta = theta.eval()\n",
    "    print(\"theta:\", best_theta)\n",
    "    print(\"beta:\", b.eval())\n",
    "\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tmp_save/save_final.ckpt\n",
      "0.8797\n"
     ]
    }
   ],
   "source": [
    "# accuracy 계산\n",
    "with tf.Session() as sess: \n",
    "    saver.restore(sess, \"./tmp_save/save_final.ckpt\")\n",
    "    y_result = y_pred.eval(feed_dict={X_input: X})\n",
    "    y_result = np.round(y_result)\n",
    "    accu = np.sum(y_result == y) / y.shape[0]\n",
    "    print(accu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 MSE =  0.286749\n",
      "Epoch: 500 MSE =  0.171616\n",
      "Epoch: 1000 MSE =  0.136134\n",
      "Epoch: 1500 MSE =  0.121854\n",
      "Epoch: 2000 MSE =  0.11416\n",
      "Epoch: 2500 MSE =  0.109288\n",
      "Epoch: 3000 MSE =  0.10587\n",
      "Epoch: 3500 MSE =  0.103302\n",
      "Epoch: 4000 MSE =  0.101275\n",
      "Epoch: 4500 MSE =  0.0996196\n",
      "Epoch: 5000 MSE =  0.0982313\n",
      "Epoch: 5500 MSE =  0.0970443\n",
      "Epoch: 6000 MSE =  0.0960141\n",
      "Epoch: 6500 MSE =  0.0951096\n",
      "Epoch: 7000 MSE =  0.094308\n",
      "Epoch: 7500 MSE =  0.093592\n",
      "Epoch: 8000 MSE =  0.0929484\n",
      "Epoch: 8500 MSE =  0.092367\n",
      "Epoch: 9000 MSE =  0.0918393\n",
      "Epoch: 9500 MSE =  0.0913583\n",
      "theta: [[ 0.95663071]\n",
      " [-2.68401504]]\n",
      "beta: 0.147815\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_epochs = 10000\n",
    "batch_size = 10000\n",
    "max_batch_idx = int( np.ceil(X.shape[0] / batch_size) )\n",
    "learning_rate = 0.01\n",
    "    \n",
    "tf.reset_default_graph()\n",
    "\n",
    "X_input = tf.placeholder(dtype=tf.float32, shape = (None, X.shape[1]), name=\"X_input\")\n",
    "y_input = tf.placeholder(tf.float32, shape = (None, 1), name=\"y_input\")\n",
    "\n",
    "with tf.variable_scope(\"logistic\") as scope:\n",
    "    theta_dim = (int(X.shape[1]),1)\n",
    "    theta = tf.get_variable(\"theta\", shape = theta_dim, \n",
    "                            initializer=tf.random_normal_initializer())\n",
    "    b =tf.get_variable(\"bias\", shape = (), initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "y_pred = logistic_regression(X_input)\n",
    "with tf.name_scope(\"loss\") as scope: \n",
    "    error = y_input - y_pred\n",
    "    mse = tf.reduce_mean(tf.square(error))\n",
    "\n",
    "\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "training_op = optimizer.minimize(mse, global_step=global_step)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver({\"weight\": theta, \"bias\": b, \"step\": global_step})\n",
    "mse_summary = tf.summary.scalar(\"MSE\", mse)\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}\".format(root_logdir, now)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess: \n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch in range(max_batch_idx):\n",
    "            X_batch, y_batch = fetcher.fetch(epoch, batch, batch_size)\n",
    "            summary_str = mse_summary.eval(feed_dict={X_input: X_batch, y_input: y_batch})\n",
    "            file_writer.add_summary(summary_str, tf.train.global_step(sess, global_step))\n",
    "            sess.run(training_op, feed_dict={X_input: X_batch, y_input: y_batch})\n",
    "        \n",
    "        if epoch % 500 == 0:\n",
    "            save_path = saver.save(sess, \"./tmp_save/save.ckpt\")\n",
    "            print(\"Epoch:\", epoch, \"MSE = \", \n",
    "                  mse.eval(feed_dict={X_input: X_batch, y_input: y_batch}))\n",
    "    save_path = saver.save(sess, \"./tmp_save/save_final.ckpt\")\n",
    "    best_theta = theta.eval()\n",
    "    print(\"theta:\", best_theta)\n",
    "    print(\"beta:\", b.eval())\n",
    "\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tmp_save/save_final.ckpt\n",
      "0.8714\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess: \n",
    "    saver.restore(sess, \"./tmp_save/save_final.ckpt\")\n",
    "    y_result = y_pred.eval(feed_dict={X_input: X})\n",
    "    y_result = np.round(y_result)\n",
    "    accu = np.sum(y_result == y) / y.shape[0]\n",
    "    print(accu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_epochs = 30000\n",
    "batch_size = 2000\n",
    "max_batch_idx = int( np.ceil(X.shape[0] / batch_size) )\n",
    "learning_rate = 0.005\n",
    "    \n",
    "tf.reset_default_graph()\n",
    "\n",
    "X_input = tf.placeholder(dtype=tf.float32, shape = (None, X.shape[1]), name=\"X_input\")\n",
    "y_input = tf.placeholder(tf.float32, shape = (None, 1), name=\"y_input\")\n",
    "\n",
    "with tf.variable_scope(\"logistic\") as scope:\n",
    "    theta_dim = (int(X.shape[1]),1)\n",
    "    theta = tf.get_variable(\"theta\", shape = theta_dim, \n",
    "                            initializer=tf.random_normal_initializer())\n",
    "    b =tf.get_variable(\"bias\", shape = (), initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "y_pred = logistic_regression(X_input)\n",
    "with tf.name_scope(\"loss\") as scope: \n",
    "    error = y_input - y_pred\n",
    "    mse = tf.reduce_mean(tf.square(error))\n",
    "\n",
    "\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "training_op = optimizer.minimize(mse, global_step=global_step)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver({\"weight\": theta, \"bias\": b, \"step\": global_step})\n",
    "mse_summary = tf.summary.scalar(\"MSE\", mse)\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}\".format(root_logdir, now)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess: \n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch in range(max_batch_idx):\n",
    "            X_batch, y_batch = fetcher.fetch(epoch, batch, batch_size)\n",
    "            if batch % 5 ==0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X_input: X_batch, y_input: y_batch})\n",
    "                file_writer.add_summary(summary_str, tf.train.global_step(sess, global_step))\n",
    "            sess.run(training_op, feed_dict={X_input: X_batch, y_input: y_batch})\n",
    "        \n",
    "        if epoch % 1000 == 0:\n",
    "            save_path = saver.save(sess, \"./tmp_save/save.ckpt\")\n",
    "            print(\"Epoch:\", epoch, \"MSE = \", \n",
    "                  mse.eval(feed_dict={X_input: X_batch, y_input: y_batch}))\n",
    "    save_path = saver.save(sess, \"./tmp_save/save_final.ckpt\")\n",
    "    best_theta = theta.eval()\n",
    "    print(\"theta:\", best_theta)\n",
    "    print(\"beta:\", b.eval())\n",
    "\n",
    "file_writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

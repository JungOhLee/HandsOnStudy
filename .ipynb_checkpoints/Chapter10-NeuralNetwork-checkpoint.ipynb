{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인공 뉴런\n",
    "## 텐서플로 고수준 API, 다층 퍼셉트론 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JOLEE/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata(\"MNIST original\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "y = y.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
    "\n",
    "import numpy as np\n",
    "shuffle_index = np.random.permutation(60000)\n",
    "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JOLEE/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[_RealValuedColumn(column_name='', dimension=784, default_value=None, dtype=tf.uint8, normalizer=None)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols = tf.contrib.learn.infer_real_valued_columns_from_input(X_train)\n",
    "feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/bp/ddf_nfxn45dc3fl_4g4pjy9m0000gn/T/tmp9f1soo5d\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1c1fd2f7b8>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/var/folders/bp/ddf_nfxn45dc3fl_4g4pjy9m0000gn/T/tmp9f1soo5d'}\n",
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JOLEE/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/JOLEE/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:192: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_global_step\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/bp/ddf_nfxn45dc3fl_4g4pjy9m0000gn/T/tmp9f1soo5d/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.64766, step = 1\n",
      "INFO:tensorflow:global_step/sec: 39.0211\n",
      "INFO:tensorflow:loss = 0.225839, step = 101 (2.564 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.9399\n",
      "INFO:tensorflow:loss = 0.193501, step = 201 (2.862 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.9846\n",
      "INFO:tensorflow:loss = 0.113124, step = 301 (2.704 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.5392\n",
      "INFO:tensorflow:loss = 0.161358, step = 401 (2.594 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.3202\n",
      "INFO:tensorflow:loss = 0.136932, step = 501 (2.610 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.0716\n",
      "INFO:tensorflow:loss = 0.0693479, step = 601 (2.495 sec)\n",
      "INFO:tensorflow:global_step/sec: 30.5517\n",
      "INFO:tensorflow:loss = 0.0937986, step = 701 (3.273 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.3026\n",
      "INFO:tensorflow:loss = 0.0898577, step = 801 (2.612 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.2662\n",
      "INFO:tensorflow:loss = 0.0428221, step = 901 (3.005 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.5987\n",
      "INFO:tensorflow:loss = 0.0400178, step = 1001 (2.890 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.156\n",
      "INFO:tensorflow:loss = 0.0433572, step = 1101 (2.554 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.6934\n",
      "INFO:tensorflow:loss = 0.0246771, step = 1201 (3.368 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.8454\n",
      "INFO:tensorflow:loss = 0.0359636, step = 1301 (2.713 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.2409\n",
      "INFO:tensorflow:loss = 0.0434501, step = 1401 (2.760 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.6922\n",
      "INFO:tensorflow:loss = 0.0384473, step = 1501 (2.725 sec)\n",
      "INFO:tensorflow:global_step/sec: 37.7792\n",
      "INFO:tensorflow:loss = 0.0175374, step = 1601 (2.648 sec)\n",
      "INFO:tensorflow:global_step/sec: 37.1663\n",
      "INFO:tensorflow:loss = 0.0184736, step = 1701 (2.690 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.4116\n",
      "INFO:tensorflow:loss = 0.0148817, step = 1801 (2.746 sec)\n",
      "INFO:tensorflow:global_step/sec: 37.5745\n",
      "INFO:tensorflow:loss = 0.0160609, step = 1901 (2.662 sec)\n",
      "INFO:tensorflow:global_step/sec: 37.7383\n",
      "INFO:tensorflow:loss = 0.0219094, step = 2001 (2.650 sec)\n",
      "INFO:tensorflow:global_step/sec: 37.0283\n",
      "INFO:tensorflow:loss = 0.0144699, step = 2101 (2.700 sec)\n",
      "INFO:tensorflow:global_step/sec: 37.9041\n",
      "INFO:tensorflow:loss = 0.0105904, step = 2201 (2.638 sec)\n",
      "INFO:tensorflow:global_step/sec: 37.8074\n",
      "INFO:tensorflow:loss = 0.0136769, step = 2301 (2.645 sec)\n",
      "INFO:tensorflow:global_step/sec: 37.6967\n",
      "INFO:tensorflow:loss = 0.00737791, step = 2401 (2.653 sec)\n",
      "INFO:tensorflow:global_step/sec: 37.0417\n",
      "INFO:tensorflow:loss = 0.0100985, step = 2501 (2.700 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.709\n",
      "INFO:tensorflow:loss = 0.0168849, step = 2601 (2.583 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.792\n",
      "INFO:tensorflow:loss = 0.00926038, step = 2701 (2.513 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.26\n",
      "INFO:tensorflow:loss = 0.00622837, step = 2801 (2.547 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.3581\n",
      "INFO:tensorflow:loss = 0.00505933, step = 2901 (2.607 sec)\n",
      "INFO:tensorflow:global_step/sec: 37.4341\n",
      "INFO:tensorflow:loss = 0.00547525, step = 3001 (2.671 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.0788\n",
      "INFO:tensorflow:loss = 0.00661756, step = 3101 (2.626 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.5818\n",
      "INFO:tensorflow:loss = 0.00627729, step = 3201 (2.527 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.6491\n",
      "INFO:tensorflow:loss = 0.00334169, step = 3301 (2.586 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.8132\n",
      "INFO:tensorflow:loss = 0.00662932, step = 3401 (2.512 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.3439\n",
      "INFO:tensorflow:loss = 0.00617986, step = 3501 (2.481 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.2529\n",
      "INFO:tensorflow:loss = 0.00547203, step = 3601 (2.835 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.3075\n",
      "INFO:tensorflow:loss = 0.00482572, step = 3701 (2.544 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.5324\n",
      "INFO:tensorflow:loss = 0.00329769, step = 3801 (2.467 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.2937\n",
      "INFO:tensorflow:loss = 0.00493644, step = 3901 (2.482 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4000 into /var/folders/bp/ddf_nfxn45dc3fl_4g4pjy9m0000gn/T/tmp9f1soo5d/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0033612.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SKCompat()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf = tf.contrib.learn.DNNClassifier(hidden_units=[300,100], n_classes=10,\n",
    "                                        feature_columns=feature_cols)\n",
    "dnn_clf = tf.contrib.learn.SKCompat(dnn_clf)\n",
    "dnn_clf.fit(scaler.transform(X_train), y_train.astype(int), batch_size=500, steps=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/bp/ddf_nfxn45dc3fl_4g4pjy9m0000gn/T/tmp9f1soo5d/model.ckpt-4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JOLEE/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.97409999999999997"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = dnn_clf.predict(scaler.transform(X_test))\n",
    "accuracy_score(y_test, y_pred['classes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 저수준 API로 심층 신경망 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망 구성 시작, 도입 부분\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28*28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 층의 연결 구현\n",
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = 2 / np.sqrt(n_inputs + n_neurons) # 표준 편차를 이렇게 잡으면 수렴이 매우 빠르다. \n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev) # 절단 정규 분포로 초기화, \n",
    "        W = tf.Variable(init, name = \"kernel\") # 가중치를 kernel 이라고 부르기도\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"bias\")\n",
    "        Z = tf.matmul(X, W) + b\n",
    "        if activation is not None:\n",
    "            return activation(Z)\n",
    "        else:\n",
    "            return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 이어진 신경망\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = neuron_layer(X, n_hidden1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "    hidden2 = neuron_layer(hidden1, n_hidden2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "    logits = neuron_layer(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 바로 위의 코드는 다음과 같은 코드다. \n",
    "# with tf.name_scope(\"dnn\"):\n",
    "#     hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "#     hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "#     logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, \n",
    "                                                              logits=logits) # 0~9사이의 정수 레이블을 labels로 받음\n",
    "    # softmax_cross_entropy_with_logits 는 label을 one-hot vector 로 받음\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1) # K번째 큰 logits 안에 label이 들어가는지 T/F 반환\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) # cast 하면 데이터 타입 바꿔줌\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.559 Validation accuracy: 0.549\n",
      "10 Train accuracy: 0.901 Validation accuracy: 0.8972\n",
      "20 Train accuracy: 0.915 Validation accuracy: 0.9144\n",
      "30 Train accuracy: 0.916 Validation accuracy: 0.9258\n",
      "40 Train accuracy: 0.939 Validation accuracy: 0.9328\n",
      "50 Train accuracy: 0.947 Validation accuracy: 0.9368\n",
      "60 Train accuracy: 0.912 Validation accuracy: 0.9404\n",
      "70 Train accuracy: 0.936 Validation accuracy: 0.9436\n",
      "80 Train accuracy: 0.936 Validation accuracy: 0.9452\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-bd5937fc12bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_examples\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0macc_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 트레이닝 변수 설정 \n",
    "n_epochs = 200\n",
    "batch_size = 1000\n",
    "\n",
    "# 훈련\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict = {X: X_batch, y: y_batch})\n",
    "        if epoch % 10 == 0:\n",
    "            acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_val = accuracy.eval(feed_dict={X: mnist.validation.images, \n",
    "                                                y: mnist.validation.labels})\n",
    "            print(epoch, \"Train accuracy:\", acc_train, \"Validation accuracy:\", acc_val)\n",
    "    save_path = saver.save(sess, \"./ch10/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./ch10/my_model_final.ckpt\n",
      "Test accuracy: 0.9783\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess: \n",
    "    saver.restore(sess, \"./ch10/my_model_final.ckpt\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: mnist.test.images, \n",
    "                                                y: mnist.test.labels})\n",
    "    print(\"Test accuracy:\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터 튜닝\n",
    "\n",
    "바꿀 수 있는 것 \n",
    "- 층 수\n",
    "- 층 별 뉴런 수\n",
    "- 활성화 함수\n",
    "- 가중치 초기화 방식 등\n",
    "\n",
    "\n",
    "### 은닉층의 수\n",
    "은닉층 하나만 해도 뉴런 수가 충분하면 아주 복잡한 함수도 모델링 가능 \n",
    "=> 보통 한 층만 있어도 잘 동작 할 수 있다. \n",
    "\n",
    "하지만 심층 신경망이 얕은 신경망보다 **파라미터 효율성**이 좋다. \n",
    "적은 파라미터로, 더 다양하게 표현, 더 빠르게 학습\n",
    "\n",
    "심층으로 해서 계층을 나누면, 저층은 비슷한 데이터 셋에서 또 사용 가능하다. \n",
    "\n",
    "### 은닉층의 뉴런 수 \n",
    "옛날에는 뉴런 수를 깔때기처럼 줄게 했다. \n",
    "300 -> 100\n",
    "\n",
    "하지만 요새는 비슷한 수로 한다. \n",
    "150 -> 150 \n",
    "\n",
    "더 간단한 방식은, 그냥 과적합 될만한 크기로 모델을 짜서, \n",
    "과대적합 되기 전에 \n",
    "학습을 조기종료 시킨다. 또 드롭아웃을 사용한다. \n",
    "==> Stretch pants 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 연습문제 9번"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JOLEE/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "\n",
    "# 자료 불러오기\n",
    "mnist = fetch_mldata(\"MNIST original\")\n",
    "X = mnist['data']\n",
    "y = mnist['target']\n",
    "y = y.astype(int)\n",
    "\n",
    "X_train, y_train, X_test, y_test = X[:60000], y[:60000], X[60000:], y[60000:]\n",
    "new_idx = np.random.permutation(60000)\n",
    "X_train = X_train[new_idx]\n",
    "y_train = y_train[new_idx]\n",
    "\n",
    "# 정규화\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 층을 만들자\n",
    "def dense_layer(input_layer, n_neurons, name, activation = None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(input_layer.get_shape()[-1])\n",
    "        std = 2 / np.sqrt(n_inputs + n_neurons)\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=std)\n",
    "        W = tf.Variable(init, name = \"weights\")\n",
    "        b = tf.Variable(0.0, name = \"bias\")\n",
    "        z = input_layer @ W + b # matmul 순서 중요\n",
    "        if activation is not None:\n",
    "            return activation(z)\n",
    "        else: \n",
    "            return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "        \n",
    "# 그래프를 만들자\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 구조를 생각해보자 hidden 3개, 300, 150, 50 으로 한 번 해보자\n",
    "\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 150\n",
    "n_hidden3 = 50\n",
    "n_class = 10\n",
    "\n",
    "with tf.name_scope(\"input\"):\n",
    "    X_input = tf.placeholder(dtype=tf.float32, shape=(None, 28*28) ,name=\"X\")\n",
    "    y_input = tf.placeholder(dtype=tf.int64, shape=(None), name=\"y\") # None, 1 로 하면 에러남\n",
    "\n",
    "hidden1 = dense_layer(X_input, n_hidden1, \"hidden1\", activation=tf.nn.relu)\n",
    "hidden2 = dense_layer(hidden1, n_hidden2, \"hidden2\", activation=tf.nn.relu)\n",
    "hidden3 = dense_layer(hidden2, n_hidden3, \"hidden3\", activation=tf.nn.relu)\n",
    "outputs = dense_layer(hidden3, n_class, \"output_logits\", activation=None)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "#     y_one_hot = tf.one_hot(y_input, depth=n_class, name=\"one_hot_labels\")\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y_input, logits = outputs)\n",
    "    loss = tf.reduce_mean(cross_entropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "with tf.name_scope(\"train\"):\n",
    "    global_step = tf.Variable(initial_value=0, trainable=False)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) # Adam 은 learning_rate 가 0.01이 default\n",
    "    training_op = optimizer.minimize(loss=loss, global_step=global_step)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    prediction = tf.argmax(outputs, axis = 1, name=\"prediction\")\n",
    "    correct = tf.equal(prediction, y_input)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, \"float\"))\n",
    "\n",
    "# Saver 생성\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FileWriter, summary 생성\n",
    "from datetime import datetime\n",
    "loss_summary = tf.summary.scalar(\"Loss\", loss)\n",
    "now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "log_dir = \"ch10/log/run-{}\".format(now)\n",
    "file_writer = tf.summary.FileWriter(log_dir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Loss: 2.60441 Training Accuracy: 0.0981\n",
      "1 Loss: 2.31875 Training Accuracy: 0.1115\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-afb36b902577>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mX_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0msummary_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_summary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1087\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \"\"\"\n\u001b[0;32m--> 531\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 훈련\n",
    "n_epoch = 100\n",
    "batch_size = 10000\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    for epoch in range(n_epoch):\n",
    "        new_idx = np.random.permutation(X_train.shape[0])\n",
    "        X_train = X_train[new_idx]\n",
    "        for batch in range(X_train.shape[0] // batch_size):\n",
    "            start = batch * batch_size\n",
    "            end = (batch+1) * batch_size\n",
    "            feed_dict = {X_input: X_train[start:end], y_input: y_train[start:end]}\n",
    "            sess.run(training_op, feed_dict)\n",
    "            if batch % 10 == 0:\n",
    "                summary_val = loss_summary.eval(feed_dict)\n",
    "                file_writer.add_summary(summary_val, tf.train.global_step(sess, global_step))\n",
    "        if epoch % 1 ==0:\n",
    "            save_path = saver.save(sess, \"./ch10/practice_model.ckpt\")\n",
    "            train_acc = sess.run(accuracy, feed_dict=feed_dict)\n",
    "            print(epoch, \"Loss:\", loss.eval(feed_dict), \"Training Accuracy:\", train_acc)\n",
    "            \n",
    "    save_path = saver.save(sess, \"./ch10/practice_model_final.ckpt\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore('./ch10/practive_model.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
